<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>DeepMind说句法偏见“帮助伯特做得更好”</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">DeepMind说句法偏见“帮助伯特做得更好”</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-05-30 05:50:51</div><div class="story_img_container"><img src="http://img.diglog.com/img/2020/5/4bf0acef46c96975d722c0c359bef6eb.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></div><div class="page_narrow text-break page_content"><p>NLP的研究人员已经知道，Google非常流行的BERT(Transformers的双向编码器表示)语言模型，基于大量数据进行训练，即使在不了解分层语法结构的情况下也能很好地执行句法语法判断任务。但它还能做得更好吗？这就是DeepMind和加州大学伯克利分校的研究人员在一项新的研究中开始发现的，该研究增加了句法偏见，以确定它们是否可以帮助伯特更好地理解，以及在哪里可以帮助伯特实现更好的理解。</p><p>该方法受到知识提炼(KD)过程的启发，该过程使用递归神经网络语法(RNNG)来提高可扩展语言模型(LMS)的句法能力。由于RNNG是从左到右预测单词的分层句法LM，因此将它们插入到在双向上下文中预测单词的BERT中是具有挑战性的。因此，研究人员创建了一种新的预训练设置，该设置提取了RNNG在上下文中的词的边缘分布，但仍然完全兼容，并保持BERT的其余部分不变，以保持其可扩展性。</p><p>在均匀分布的双向背景下提取RNNG的近似边缘(“UF-KD”)。</p><p>提取RNNG在双向背景下的近似边缘，并采用一元分布(“UG-KD”)。</p><p>研究人员在六个不同的结构化预测任务上评估了他们的结构提取的BERT，包括句法、语义和共指解析，以及流行的GLUE(通用语言理解评估)基准。</p><p>测试结果表明，所有四个结构提取的BERT模型在相对错误率降低2-21%的同时，性能都一致优于标准的BERT基线。</p><p>研究结果表明，句法归纳偏向对包括非句法预测任务在内的各种结构化预测任务都是有益的，而且这些偏向还可以提高下游任务的微调样本效率。</p><p>研究人员提出，未来的潜在研究可以着眼于设计易于扩展的模型，将更强的结构性偏见概念整合在一起。</p></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/syncedreview/deepmind-says-syntactic-biases-helped-bert-do-better-c4fe8bf2e11d">https://medium.com/syncedreview/deepmind-says-syntactic-biases-helped-bert-do-better-c4fe8bf2e11d</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/伯特/">#伯特</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/syntactic/">#syntactic</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/句法/">#句法</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/病毒/">#病毒</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/冠状病毒/">#冠状病毒</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/app/">#app</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>