<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>网格世界中的强化学习算法</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1>网格世界中的强化学习算法</h1><div class="row"><div class="col-lg-8 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time">2020-05-05 18:40:08</div><p>在这本笔记本中，我们反复求解一个简单的环境，探索以下算法类的特点：{线性规划(LP)、动态规划(DP)、蒙特卡罗(MC)和时差学习(TD)}。对于MC和TD，我们还在预测、按策略控制和非策略控制方面探索了更多的问题空间。</p><p>from__Future__import print_function from IPython.display导入Display，clear_output from datetime import datetime，timeDelta from itertools从PANDA导入产品。ploting import register_matplotlib_Converters导入ipywidget与np一样导入umpy作为nP导入matplotlib.pylot作为PLT导入熊猫作为PD导入随机导入TensorFlow作为TF PD。选项。显示。MAX_COLUMNS=NONE REGISTER_matplotlib_Converters()%matplotlib内联。</p><p>%pip install ortools from ortools.line_solver import pyspraplp#首次安装后需要重新启动笔记本电脑_=pyspraplp。求解器(&#39；LinearProgrammingGridWorld&#39；，pyraplp)。求解器。GLOP_LINEAR_PROGRAM)。</p><p>在索引中查找：https://pypi.org/simple，https://pypi.lyft.net/pypi/Requirement已经满足：/CODE/venvs/venv/lib/python3.6/site-Packages(7.6.7691)中的ortools已经满足要求：/code/venvs/venv/lib/python3.6/site-Packages(From Ortools)(3.11.3)中的协议buf&gt；=3.11.2in/code/venvs/venv/lib/python3.6/site-Packages(From Ortools)(3.11.3)已经满足要求：6&gt；=1.10 in/code/venvs/venv/lib/python3.6/site-Packages(From Ortools)(1.12.0)已满足要求：/code/venvs/venv/lib/python3.6/site-Packages(from protocol buf&gt；=3.11.2-&gt；or tools)(45.2.0)注意：您可能需要重新启动内核才能使用更新的软件包。</p><p>#最优策略，Gamma=0.9 Expect_Values=Np。array([[22.0，24.4，22.0，19.4，17.5]，[19.8，22.0，19.8，17.8，16.0]，[17.8，19.8，17.8，16.0，14.4]，[16.0，17.8，16.0，14.4，13.0]，[14.4，16.0，14.4，13.0，11.7])。</p><p>从枚举导入枚举类操作(枚举)：北=&#39；N&#39；，0，λx：(X[0]-1，x[1])，(0，1)东=&#39；E&#39；1，λx：(x[0]，x[1]+1)，(1，0)西=&#39；W&#39；，2，λx：(X[0]，x[1]-1)，(-1，0)South=&#39；S&#39；，3，λx：(x[0]+1，x[1])，(0，-1)def__init__(self，display_name，index，Transition，Direction)：self。DISPLAY_NAME=显示名称自身。INDEX=索引自身。TRANSPION=TRANSPION#Numpy行-主坐标SELF。方向=方向#matplotlib坐标def__str__(Self)：返回自身。显示名称。</p><p>类Agent(Object)：def__init__(self，name)：self。name=name def policy(self，state)：&#34；&#34；&#34；&#34；返回给定状态的操作&#34；&#34；&#34；引发NotImplementedError()。</p><p>GreedyAgent类(代理)：&#34；&#34；&#34；&#34；这个代理拥有关于环境转换概率和奖励的完整信息。这种方法对于环境未知的复杂问题是不可行的。&#34；&#34；&#34；def__init__(SELF，STATE_VALUES=NONE，NAME=&#39；OPTIME&#39；，GAMMA=0.9)：SUPER()。__init__(Name)如果STATE_VALUES为NONE：STATE_VALUES=NP。随机的。Randn(5，5)Self。STATE_VALUES=STATE_VALUES#v(S)自身。Gamma=Gamma Self。Environment=GridWorld()def policy(self，state)：&#34；&#34；&#34；&#34；使用确切的过渡/奖励结果进行策略迭代。&#34；&#34；&#34；action=one max_action_value=-1000表示正在执行操作：自我。环境。set_state(State)s1，r=self。环境。转换(A)action_value=r+self。伽马*自我。STATE_VALUES[S1]if action_value&gt；max_action_value：action=a max_action_value=action_value返回操作</p><p>类EpsilonGreedyAgent(GreedyAgent)：&#34；&#34；&#34；&#34；带有epsilon贪婪探索的GreedyAgent。这种方法对于环境未知的复杂问题是不可行的。&#34；&#34；&#34；def__init__(self，epsilon=1e-2，**kwargs)：Super()。__init__(**kwargs)自我。epsilon=epsilon定义策略(自身，状态)：如果是随机的。随机()&lt；自身。埃普西隆：随机返回。CHOICE([a for a in Action])返回SUPER()。策略(州)。</p><p>类LearnedAgent(Agent)：&#34；&#34；&#34；&#34；从随机策略开始学习最优策略，并且没有先验记录。&#34；&#34；&#34；def__init__(self，name=&#39；Learning&#39；，Gamma=0.9，dim=5，epsilon=0)：Super()。__init__(名称)自身。ACTION_VALUES=NP。随机的。Randn(5，5，4)#q(s，a)self。_POLICY={s：随机。产品(Range(DIM)，Range(DIM))}Self的选择([a for a in Action])。Gamma=Gamma Self。epsilon=epsilon定义策略(自身，状态)：如果是随机的。随机()&lt；自身。埃普西隆：随机返回。Choose([a for a in Action])返回Self。_policy[state]def update_state_policy(self，s)：old_action=self。_POLICY[s]STRATE=True max_action_value=-1000 for a in Action：action_value=self。ACTION_VALUES[(s[0]，s[1]，a.。index)]if action_value&gt；max_action_value：action=a max_action_value=action_value if action！=old_action：policy_Stability=false self。_POLICY[s]=action return稳定定义POLICY_IMPORT(SELF)：&#34；&#34；&#34；根据我们最新的ACTION_VALUES&#34；&#34；&#34；POLICY_STRATE=Self中s的True更新我们的策略。_策略。KEYS()：STRATE=SELF。UPDATE_STATE_POLICY如果不稳定：POLICY_STRATE=FALSE RETURN POLICY。</p><p>类GridWorld(Object)：def__init__(self，initial_state=(0，0))：self。DIM=5自身。STATE=初始状态定义SET_STATE(SELF，STATE)：SELF。STATE=STATE DEF_TRANSION_STATE(SELF，ACTION)：&#34；&#34；&#34；&#34；&#34；&#34；&#34；x，y=ACTION。过渡(自我。状态)如果x&lt；0或x&gt；=SELF。戴姆：回归自我。状态为y&lt；0或y&gt；=self。戴姆：回归自我。状态返回x，y定义转换(Self，action)：&#34；&#34；&#34；&#34；&#34；&#34；&#34；&#34；&#34；&#34；&#34；&#34；如果Self，则&#34；&#34；&#34；reward=0。STATE==(0，1)：#特殊状态A NEW_STATE=(4，1)REWARY=10 ELIF SELF。STATE==(0，3)：#特殊状态B NEW_STATE=(2，3)REWARY=5 ELSE：NEW_STATE=SELF。_TRANSPATION_STATE(Action)#如果NEW_STATE==SELF，则移动无效。状态：奖励=-1自我。STATE=NEW_STATE返回NEW_STATE，奖励。</p><p>给定教科书上的解决方案，我们的代理通常直接移动到位置(1，4)来领取10分奖励。</p><p>定义DISPLAY_STATE_VALUES(STATE_VALUES，标题=无)：FIG，AX=PLT。子图(图大小=(4.8，4.8))，如果标题：AX。set_title(Title)#将行较长的umpy坐标转换为matplotlib坐标Transform_Values=np。rot90(STATE_VALUES。t)im=ax。imshow(Transform_Values，Cmap=&#39；hot&39；)图。产品(范围(5)，范围(5))中s的ColorBar(im，ax=ax)agent=GreedyAgent(State_Value)：action=agent。策略dx，dy=操作。方向#将行数较多的多个坐标变换为matplotlib坐标x，y=s[1]，4-s[0]ax。箭头(x-0.25*dx，y-0.25*dy，0.5*dx，0.5*dy，head_width=0.05)轴。set_xlim(-0.5，4.5)ax。set_ylim(-0.5，4.5)PLT。show()def display_state(state，fig=NONE，AX=NONE)：如果AX==NONE：FIG，AX=PLT。子图(图大小=(6.4*2，4.8))GRID=NP。零((5，5))栅格[STATE[0]，STATE[1]]=1 AX。imshow(grid，cmap=#39；viridis；)display(图)Clear_Output(Wait=True)PLT。show()。</p><p>精确的解决方案，但对公式、超参数极其敏感，并且不能扩展到除了最简单的问题之外的任何问题，而不会进行剧烈的简化/假设。</p><p>def SOLVE_OPTIMIZATION(Gamma=0.9，Dim=5)：解算器=pyspraplp。求解器(&#39；LinearProgrammingGridWorld&#39；，pyraplp)。求解器。GLOP_LINEAR_PROGRAM)STATE_VALUES={s：求解器。NumVar(-求解器。无穷()，求解器。产品(range(Dim)，range(Dim))}Objective=求解器中s的infinity()，f&#39；state_{s[0]}_{s[1]}&#39；)。目的()目的。SetMinimization()Environment=GridWorld()#目标：最小化STATE_VALUES中s0，v_s0的所有s的sum(v(S))。项目()：客观。设置系数(v_s0，1)#最优：v(S)=max_a{r+γ*v(s&</p><p>使用梯度下降和围绕v(S)-v*(S)之间的误差定义的损失(这在很大程度上要归功于Gautam Kedia清理了损失函数)。</p><p>TF。配置。实验性Run_Functions_Eagerly(True)@Tf。函数def get_Problem(Gamma，Constraint_Payment，STATE_VALUES=NONE，DIM=5)：#TODO：如果STATE_VALUES==NONE：STATE_VALUES={s：tf，则设置更好的初始值。变量(np.。随机的。Rand()，name=f&#39；state_{s[0]}_{s[1]}&#39；，dtype=tf。Float32)for s in product(range(Dim)，range(Dim))}Errors=[]Environment=GridWorld()for s0，v_s0 in state_value。Items()：ACTION_VALUES=[]for a in Action：#确定性地将(s，a)应用于GridWorld以获取(s&#39；，r)环境。set_state(S0)s1，r=环境。转换(A)v_s1=STATE_VALUES[S1]ACTION_VALUE=r+γ*v_S1 ACTION_VALUES。追加(action_value-v_s0)错误。追加(Tf.。Reduce_max(Action_Values)目标=Tf。REDUTE_SUM(TF.。Square(错误)，name=&#39；Objective&#39；)返回目标，STATE_VALUES定义PLOT_LP_STATE_VALUES(STATE_VALUES_DICT，TITLE=NONE)：LP_VALUES=NP。对于(x，y)，STATE_VALUES_DICT中的变量为零((5，5))。Items()：lp_values[x，y]=var。Numpy()DISPLAY_STATE_VALUES(LP_VALUES，TITLE=TITLE)返回LP_VALUES def solve_gradients_tf(伽马=0.9，约束_罚金=10，迭代=50000，停止_增量=1e-4，对数频率=NONE)：如果LOG_FREQUENCE为NONE：LOG_FREQUENCE=迭代+1优化器=TF。眼镜蛇。优化器。ADAM()PRIV_LOSS_VALUE=1e9 LOSS_HISTORY=[]Objective，STATE_VALUES=Get_Problem(Gamma=Gamma，Constraint_Payment=Constraint_Payment)，范围(1，迭代)：使用TF。GradientTape()AS TAPE：OBJECT，STATE_VALUES=GET_PROBUCT(GAMA=GAMA，CONSTRAINT_PINDITY=CONSTRAINT_PINDY，STATE_VALUES=STATE_VALUES)。值())损失=目标梯度=磁带。梯度(Lost，Training_vars)优化器。应用梯度(zip(grads，Training_vars))LOSS_VALUE=LOSS。Numpy()LOSS_HISTORY。APPEND(LOSS_VALUE)IF I%LOG_FREQUENCE==0：PRINT(f&#39；Iteration{i}：Loss={Lost_Value}&#39；)PLOT_LP_STATE_VALUES(STATE_VALUES，Title=f&#39；Iteration{i}&#39；)If abs(prev_Loss_Value-Loss_Value)&lt；STOPING_DELTA：Break PRIV_LOSS_VALUE=LOSS_VALUE RETURN STATE_VALUES。</p><p>#去喝杯咖啡，这将需要大约20分钟的时间，LP_STATE_VALUES=SOLVE_GRANDIONS_TF(GAMMA=0.9，STOPING_DELTA=0.1)PLOT_LP_STATE_VALUES(LP_STATE_VALUES，Title=&#39；TensorFlow梯度下降&#39；)。</p><p>Array([[21.97758293，24.41950417，21.97759247，19.41941071，17.47755814]，[19.77989769，21.97759819，19.77980042，17.801651，16.02147293]，[17.80190659，19.77986145，17.8018322，16.02175903，14.41941547]，[16.02163315，17.80198097，16.02163696，14.41940784，12.97757339]，[14.41948891，16.021698，14.41958618，12.97769833，12.97769833]])。</p><p>数组([[22.。、24.4、22。，19.4，17.5]，[19.8，22.。，19.8，17.8，16.]，[17.8，19.8，17.8，16.，14.4]，[16，17.8，16.，14.4，13.]，[14.4，16.，14.4，13.，11.7])。</p><p>另一种找到精确解决方案的策略。这还需要对环境有完美的了解，并且不能扩展到复杂的问题。它的收敛性保证也比线性规划弱。动态规划可以处理比线性规划更大的问题，但是状态空间呈指数增长，我们必须使用其他方法来解决最大的问题。</p><p>def value_iteration(Gamma=0.9，ε=1e-4)：STATE_VALUES=Np。随机的。RANDN(5，5)DIM=5环境=GridWorld()Delta=1000而Delta&gt；epsilon：产品中S0的增量=0(Range(DIM)，Range(DIM))：V_S0=STATE_VALUES[S0]#策略：ARG-MAX ACTION-VALUE，具有关于环境OPTIME_ACTION_VALUE=-1000的完美信息在操作中：#确定性地将(s，a)应用于Gridworld环境。set_state(S0)s1，r=环境。转换(a。</p><p>Array([[21.97747629，24.4194222，21.97747998，19.4194222，17.47747998]，[19.77972866，21.97747998，19.77973198，17.80175878，16.0215829]，[17.8017558，19.77973198，17.80175878，16.0215829，14.41942461]，[16.02158022，17.80175878，16.0215829，14.41942461，12.97748215]，[14.4194222，16.0215829，14.41942461，12.97748215，12.97748215]])</p><p>蒙特卡罗算法是第一类可扩展到复杂现实问题的算法。与以前的方法不同，蒙特卡罗方法不需要环境模型，既可以从模拟事件中学习，也可以从实际经验中学习。此外，如果只有特定区域与我们的用例相关，则此策略不需要对每个州进行精确估计。</p><p>然而，蒙特卡洛需要一种生成情节的策略，无论是针对策略上的学习、策略外的学习、模拟还是回测。如果没有某种形式的探索，例如ε-贪婪策略，这些算法都不会在合理的时间内产生合理的结果。这是一个极其强烈的迹象，表明有监督的学习方法会天真地达到次优策略，而永远找不到逃离局部极小值所需的数据。</p><p>#TODO：提前停止def MC_Prediction(EVERY_ACVISE=FALSE，GAMA=0.9，DIM=5，T=32，Iterations=int(1e4)，LOG_FREQUENCE=NONE)：如果LOG_FREQUENCE为NONE：LOG_FREQUENCE=Iterations+1#GreedyAgent在这里严重失败(可能监督学习也会失败)agent=EpsilonGreedyAgent(name=&#39；第一值MC预测&#39；)返回{s：(0，0)for s in product(range(Dim)，range(Dim))}Environment=GridWorld()for i in range(1，iterations)：if i%log_Frequency==0：display_state_value(代理。STATE_VALUES，Title=f&#39；MC预测：迭代{i}&#39；)#模拟剧集S0=(随机。随机数(0，4)，随机数。RANDINT(0，4))环境。SET_STATE(S0)插曲=[S0]#S0，a0，s1，r1，a1，s2，r2，a2，s3.。FOR_IN范围(T)：A0=座席。策略(S0)S1，R1=环境。过渡(A0)集。EXTEND([a0，s1，r1])s0=s1#当j&gt时，从剧集结尾ret=0 j=T-1 SEPTION_RETURNS={}开始回放([a0，s1，r1])s0=s1#；0：r1=SEPTION[3*j+3]s0=SEPTION[3*j-1]j-=1 ret=r1+Gamma*ret SESSION_RETURNS[s0]=ret if Every_access：ean_ret，c=Returns[s0]返回[s0]=((ean_ret*c+ret)/(c+1)，c+1)#更新SESSION_RENS中s，ret的状态值v(S)。Items()：如果不是EVERY_ACCESS：Mean_ret，c=返回[s]返回[s]=((Mean_ret*c+ret)/(c+1)，c+1)agent。STATE_VALUES[s]=退货[s][0]退货代理。</p><p>Array([[16.88336638，19.11593205，15.90961158，14.10295261，12.66921586]，[10.19679358，16.39664102，9.64112818，12.32284046，7.02133044]，[12.84864394，14.33527863，7.98370027，10.955611，5.55804087]，[6.35501717，12.15020809，7.09423013，9.42028638，8.57767909]，[5.13072527，10.60466385，5.11104244，8.28273845，8.28273845]])。</p><p>使用本期节目中所有可用的示例，而不是仅针对每个州使用最早的示例。换句话说，使用更多的数据来估计州价值，即使这一集接近尾声的例子不是非常精确的估计。</p><p>Array([[15.75148645，14.64272644，15.7478293，13.97882859，10.39855134]，[13.88160682，10.6009308，13.97915625，12.37753395，10.17550269]，[6.95911765，8.5517532，8.20427254，10.83583432，8.98310377]，[8.1790024，7.07004833，3.72116253，9.01227403，7.80683278]，[4.35314417，5.67303207，2.75351299，7.98036612，7.98036612]])。</p><p>这是我们的分析中第一个不需要准确的环境模型的例子。相反，我们使用我们的代理与环境交互以生成新的样本。换句话说，我们通过影响用户来学习，不管是好是坏，即使这意味着在短期内给我们的用户带来可怕的体验。</p><p>def MC_CONTROL(EVERY_ACCESS=FALSE，GAMA=0.9，DIM=5，T=32，Iterations=int(1e4)，log_Frequency=None)：如果log_Frequency为NONE：LOG_FREQUENCE=Iterations+1 agent=LearnedAgent(Name=&#39；MC Control&#39；，epsilon=0.01)#这不会&#39；t在没有某些探索的情况下工作返回={(x，y，a)：(0，0)for x，y，a in product(range(Dim)，range(Dim)，[a for a in action])}Environment=Gridworld()for i in range(1，迭代数)：if i%log_Frequency==0：display_state_value(代理。ACTION_VALUES。Max(轴=2)，title=f&#39；MC控制：迭代{i}&#39；)#从随机(状态，动作)x，y，a0=(随机.。随机数(0，4)，随机数。随机数(0，4)，随机数。选择([a for a in Action]))s0=(x，y)环境。SET_STATE(S0)插曲=[S0]#S0，a0，s1，r1，a1，s2，r2，a2，s3.。FOR_IN范围(T)：S1，R1=环境。过渡(A0)集。EXTEND([a0，s1，r1])a0=座席。POLICY(S1)#从剧集结尾向后回放ret=0 j=T-1 SEPTION_RETURNS={</p><p>..</p><div class="text-break sotry_link"><a target="_blank" href="https://nbviewer.jupyter.org/url/github.com/laxatives/rl/blob/master/gridworld_mdp.ipynb">https://nbviewer.jupyter.org/url/github.com/laxatives/rl/blob/master/gridworld_mdp.ipynb</a></div><div class="story_tags"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/state/">#state</a></button></div></div></div><div class="col-lg-4 col-0"><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/百度/">#百度</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/搜索/">#搜索</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button></div></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>