<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>从头开始：反向模式自动区分(Python中)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">从头开始：反向模式自动区分(Python中)</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-06-15 02:35:02</div><div class="page_narrow text-break page_content"><p>自动区分是深度学习框架的基础。深度学习模型通常使用基于梯度的技术进行训练，Autodiff使得即使是从巨大的、复杂的模型中也很容易获得梯度。“反向模式自动比较”是大多数深度学习框架使用的自动比较方法，因为它的效率和准确性。</p><p>小型autodiff框架将处理标量。我们可以(稍后)使用NumPy将其矢量化。</p><p>术语说明：从现在开始，“autodiff”将指“反向模式autodiff”。“Gradient”用法松散，但在本文中通常指“一阶偏导数”。</p><p>a=4 b=3 c=a+b#=4+3=7 d=a*c#=4*7=28。</p><p>问题1：$d$相对于$a$的梯度是多少，即$\frac{\Partial{d}}{\Partial{a}}$？(来试试这个吧！)。</p><p>有很多方法可以求解Q1，但我们使用乘积规则，即如果$y=x_1x_2$，则$y‘=x_1’x_2+x_1x_2‘$。</p><p>phew…。如果你想知道$\frac{\Partial{d}}{\Partial{b}}$，你必须重新执行这个过程。</p><p>现在我们来看一下解决Q1问题的自动求差方法。以下是一个数字：</p><p>在左边，我们看到系统用图形表示。每个变量都是一个节点；例如，$d$是最上面的节点，$a$和$b$是最下面的叶节点。</p><p>在右边，我们从autodiff的角度看系统。让我们将图边上的值称为局部导数。通过使用局部导数和简单的规则，我们将能够计算出我们想要的导数。</p><p>这是Q1的答案，用自动差分法计算。你能看出它与这个数字有什么关系吗？</p><p>我们通过查找从$d$到$a$(不与虚线箭头相反)的路由，然后应用以下规则，从图中获得此答案：</p><p>第一条路线是直接从$d$到$a$，这给我们提供了$\frac{\Partial{\bar{d}{\Partial{a}}$Term。第二条路线是从$d$到$c$再到$a$，这给出了术语$\frac{\Partial{\bar{d}{\Partial{c}}*\frac{\Partial{\bar{c}{\Partial{a}}$。</p><p>我们的autodiff实现将沿着图向下，计算$d$相对于每个子节点的导数，而不是像我们刚才对$d$所做的那样，只计算特定节点的导数。请注意，我们可以计算$d$相对于$c$和$b$的梯度，而不需要太多的工作。</p><p>我们在上面的图形边上看到了“本地导数”，其形式为：$\frac{\Partial\bar{y}}{\Partial x}$。</p><p>一般而言：要获得局部导数，请将进入节点的变量视为不是其他变量的函数。</p><p>例如，回想一下$d=a*c$。然后将$\frac{\Partial{d}}{\Partial{a}}=2a+b$与$\frac{\Partial\bar{d}}{\Partial a}=c$进行比较。局部导数$\frac{\PARTIAL\BAR{d}}{\PARTIAL a}=c$，是在对$d$的表达式进行微分之前将$c$视为常量而获得的。</p><p>定义简单函数的局部导数通常很容易，如果您知道局部导数，那么将函数添加到autodiff框架中也很容易。例如：</p><p>加法：$n=a+b$，本地导数为：$\frac{\Partial\bar{n}}{\Partial a}=1$和$\frac{\Partial\bar{n}}{\Partial b}=1$。</p><p>乘法：$n=a*b$，本地导数为：$\frac{\Partial\bar{n}}{\Partial a}=b$和$\frac{\Partial\bar{n}}{\Partial b}=a$。</p><p>函数get_gradients使用节点的渐变数据递归地遍历图形，计算渐变。它使用我们在上面看到的规则：</p><p>堆栈中的元组(在get_gradients中)类似于grad中的元组，但是它们包含当前路由值，而不是局部导数值。</p><p>从集合导入defaultdict类Var：&#34；&#34；&#34；一个叶节点(没有子节点)。&#34；&#34；&#34；def__init__(self，value)：self。value=value#节点的标量值。类add：&#34；&#34；&#34；&#34；&#34；&#34；&#34；&#34；def__init__(self，a，b)：self。值=a。值+b。重视自我。grad=[(a，1)，(b，1)]#子节点&amp；对应&#39；本地导数&39；class mul：&#34；&#34；&#34；&#34；&#34；&#34；def__init__(self，a，b)：self。值=a。值*b。重视自我。梯度=[(a，b.。值)、(b，a。value)]def get_gradients(Parent_Node)：&#34；&#34；&#34；&#34；往下走，计算`parent_node`对每个节点的导数。&#34；&#34；&#34；&#34；gradients=defaultdict(lambda：0)stack=parent_node。格拉德。copy()#(节点，ROUTE_VALUE)元组列表。而STACK：NODE，ROUTE_VALUE=STACK。POP()梯度[node]+=route_value#&#34；将不同的路由相加。&#34；如果没有isinstance(node，Var)：#如果节点有子节点，则将它们放到堆栈中。对于CHILD_NODE，节点中的CHILD_ROUTE_VALUE。葛兰德：堆叠。APPEND((CHILD_NODE，CHILD_ROUTE_VALUE*ROUTE_VALUE))#&#34；将路径的边相乘。&#34；RETURN DICT(渐变)。</p><p>a=Var(4)b=Var(3)c=add(a，b)#=4+3=7 d=mul(a，c)#=4*7=28渐变=get_gradients(D)print(&#39；d.value=&#39；，d..。值)打印(&#34；d关于a的偏导数=&#34；，梯度[a])</p><p>类操作：&#34；&#34；&#34；允许使用+、*、-等。&#34；&#34；&#34；def__add__(self，ther)：return add(self，Other)def__mul__(self，ther)：return MUL(self，Other)def__sub__(self，Other)：return add(self，Neg(Other))def__truediv__(self，Other)：return Mul(self，Inv(Other))class Var(Ops)：def__init_(self，value)：self。value=值类添加(操作)：def__init__(self，a，b)：self。值=a。值+b。重视自我。grad=[(a，1)，(b，1)]class mul(Ops)：def__init__(self，a，b)：self。值=a。值*b。重视自我。梯度=[(a，b.。值)、(b，a。value)]class neg(运算)：def__init__(self，var)：self。值=-1*var。重视自我。grad=[(var，-1)]类存货(操作)：def__init__(self，var)：self。值=1/var。重视自我。grad=[(var，-var.。值**-2)]。</p><p>我们可以从我们添加到框架中的函数中获得任意函数的梯度。例如：</p><p>a=Var(230.3)b=Var(33.2)def(a，b)：return(a/b-a)*(b/a+a+b)*(a-b)y=f(a，b)梯度=获取梯度(Y)print(&#34；y关于a的偏导数=&#34；，梯度[a])print(&#34；y关于b的偏导数=&#34；，渐变[b])。</p><p>y对a的偏导数=-153284.83150602411 y对b的偏导数=3815.0389441500993。</p><p>Δ=Var(1e-8)Numerical_grad_a=(f(a+δ，b)-f(a，b))/Delta Numerical_grad_b=(f(a，b+Delta)-f(a，b))/Delta Print(&#34；a=#34；，Numerical_grad_a的数值估计。value)打印(&#34；b=&#34；，Numerical_grad_b.。值)。</p><p>将numpy导入为NP类Sin(Ops)：def__init__(self，var)：self。值=NP。Sin(var.。值)自我。梯度=[(var，np。Cos(var.。value))]class Exp(Ops)：def__init__(self，var)：self。值=NP。EXP(变量。值)自我。grad=[(var，sel.。value)]类日志(Ops)：def__init__(self，var)：self。值=NP。LOG(变量。值)自我。grad=[(var，1./var.。值)]。</p><p>A=Var(43.。)。B=Var(3.)。C=Var(2.)。def(a，b，c)：F=Sin(a*b)+Exp(c-(a/b))返回Log(f*f)*Cy=f(a，b，c)梯度=get_gradients(Y)print(&#34；y关于a的偏导数=&#34；，梯度[a])print(&#34；y关于b的偏导数=&#34；，渐变[b])打印(&#34；y关于c的偏导数=&#34；，渐变[c])。</p><p>y对a的偏导数=60.85353612046653 y对b的偏导数=872.2331479536114 y对c的偏导数=-3.2853671032530305</p><p>Delta=Var(1e-8)Numerical_grad_a=(f(a+Delta，b，c)-f(a，b，c))/Delta Numerical_grad_b=(f(a，b+Delta，c)-f(a，b，c))/Delta Numerical_grad_c=(f(a，b，c+Delta)-f(a，b，c))/Delta print(&#34；a=#34；，Numerical_grad_a的数值估计。value)打印(&#34；b=&#34；，Numerical_grad_b.。value)打印(&#34；c=&#34；，Numerical_grad_c的数值估计。值)。</p><p>a=60.85352186602222的数值估计b=872.232160009645 c=-3.285367089489455的数值估计。</p><p>对我们的最小框架最有效的补充将是向量化(但不是以下面的方式执行)。</p><p>我们将研究一种计算效率极低的方法来向量化我们的autodiff框架。(不推荐将其用于任何事情，因为它太慢了。)。</p><p>将numpy导入为np to_var=np。Vector torize(lambda x：var(X))#将NumPy数组转换为Var对象数组TO_VERVIES=NP。矢量化(λ变量：变量。value)#从Var对象数组中获取值。</p><p>将matplotlib.pylot导入为PLT def update_weights(权重、渐变、速率)：对于范围内的i(权重。Shape[0])：对于范围内的j(权重。Shape[1])：权重[i，j]。value-=lrate*梯度[权重[i，j]]np。随机的。SEED(0)INPUT_SIZE=50 OUTPUT_SIZE=10 LARATE=0.001 x=TO_VAR(NP。随机的。RANDOM(INPUT_SIZE))y_TRUE=to_var(np.。随机的。随机(Output_Size))权重=to_var(np.。随机的。随机((INPUT_SIZE，OUTPUT_SIZE))，对于范围(100)内的i，LASS_VALUES=[]：Y_PRED=Np。点(x，重量)损失=Np。SUM((y_true-y_pred)*(y_true-y_pred))LOSS_VALUES。追加(损失。值)渐变=get_gradients(损耗)update_weights(权重、渐变、速率)PLT。绘制(亏损)PLT。xlabel(&#34；时间点&#34；)；PLT。ylabel(&#34；亏损&34；)；PLT。标题(#34；单线性层学习)PLT。show()。</p><p>马上就要来了：第二部分，我们将研究如何更高效地向量化我们的最小框架。</p><p>份额：</p></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://sidsite.com/posts/autodiff/">https://sidsite.com/posts/autodiff/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/python/">#python</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/反向/">#反向</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/reverse/">#reverse</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/var/">#var</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1006512.html"><img src="http://img.diglog.com/img/2020/6/thumb_dddef8917724a3b9dbc999f627035ebf.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1006512.html">用VTK实现3D对象科学可视化的Python类</a></div><span class="my_story_list_date">2020-6-14 15:42</span></div><div class="col-sm"><div><a target="_blank" href="/story/1006506.html"><img src="http://img.diglog.com/img/2020/6/thumb_81bf5e040828749ba03f06d9e734c486.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1006506.html">热插拔Python 3代码</a></div><span class="my_story_list_date">2020-6-14 13:59</span></div><div class="col-sm"><div><a target="_blank" href="/story/1006473.html"><img src="http://img.diglog.com/img/2020/6/thumb_a98acebf44ce873a1a02db66d68457ce.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1006473.html">Python会取代Java吗？JetBrains发布“开发者生态系统状况”调查</a></div><span class="my_story_list_date">2020-6-14 7:39</span></div><div class="col-sm"><div><a target="_blank" href="/story/1006250.html"><img src="http://img.diglog.com/img/2020/6/thumb_139b74ead05f3327f4e19cdc593f1f8a.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1006250.html">使用此Python加密算法永远不会忘记您的密码</a></div><span class="my_story_list_date">2020-6-12 10:16</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/病毒/">#病毒</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/冠状病毒/">#冠状病毒</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/app/">#app</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>